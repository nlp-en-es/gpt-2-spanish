# GPT-2 Spanish

GPT-2 model pre-trained from scratch using the Spanish portion of OSCAR during the [Flax x Hugging Face](https://discss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104) community event by
[@mariagrandury](https://github.com/mariagrandury),
[@mrm8488](https://github.com/mrm8488),
[@pablogps](https://huggingface.co/pablogps),
[@daveni](https://huggingface.co/daveni),
[@srisweet](https://huggingface.co/srisweet),
[@jdposa](https://huggingface.co/jdposa),
[@shpotes](https://huggingface.co/shpotes), and
[@jorgealro](https://huggingface.co/jorgealro).


## Model description

The model used for training is [OpenAI's GPT-2](https://openai.com/blog/better-language-models/), introduced in the paper ["Language Models are Unsupervised Multitask Learners"](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and Ilya Sutskever.

This model is available in the ðŸ¤— [Model Hub](https://huggingface.co/gpt2).

## Intended uses & limitations

### How to use (TODO)

### Limitations and bias (TODO)

## Training data

Spanish portion of OSCAR or **O**pen **S**uper-large **C**rawled **A**LMAnaCH co**R**pus, a huge multilingual corpus obtained by language classification and filtering of the [Common Crawl](https://commoncrawl.org/) corpus using the [goclassy](https://github.com/pjox/goclassy) architecture.

This corpus is available in the ðŸ¤— [Datasets](https://huggingface.co/datasets/oscar) library.

### Training procedure (TODO)

### Eval results (TODO)
